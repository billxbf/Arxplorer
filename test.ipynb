{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from arxplore.datamodel import Feed, Config\n",
    "\n",
    "\n",
    "def parse_feed(namespace: str, config: Config) -> List[Feed]:\n",
    "    url = f\"https://arxiv.org/list/{namespace}/new\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    feeds = []\n",
    "    for entry in soup.find_all(\"dl\"):\n",
    "        title = entry.find(\"div\", {\"class\": \"list-title\"}).text\n",
    "        abstract = entry.find(\"div\", {\"class\": \"abstract\"}).text\n",
    "        authors = entry.find(\"div\", {\"class\": \"list-authors\"}).text\n",
    "        url = entry.find(\"a\", {\"title\": \"Abstract\"}).get(\"href\")\n",
    "        feed = Feed(title=title, abstract=abstract, authors=authors, url=url)\n",
    "        feeds.append(feed)\n",
    "    return feeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = \"cs.AI\"\n",
    "url = f\"https://arxiv.org/list/{namespace}/new\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved the page\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the response is successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully retrieved the page\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page, status code: {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_blocks = soup.find_all('dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dt><a name=\"item1\">[1]</a>   <span class=\"list-identifier\"><a href=\"/abs/2402.09413\" title=\"Abstract\">arXiv:2402.09413</a> [<a href=\"/pdf/2402.09413\" title=\"Download PDF\">pdf</a>, <a href=\"/ps/2402.09413\" title=\"Download PostScript\">ps</a>, <a href=\"/format/2402.09413\" title=\"Other formats\">other</a>]</span></dt>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper = paper_blocks[0]\n",
    "paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Mathematical Explanations\n",
      "Authors: Joseph Y. Halpern\n",
      "Abstract: A definition of what counts as an explanation of mathematical statement, and\n",
      "when one explanation is better than another, is given. Since all mathematical\n",
      "facts must be true in all causal models, and hence known by an agent,\n",
      "mathematical facts cannot be part of an explanation (under the standard notion\n",
      "of explanation). This problem is solved using impossible possible worlds.\n",
      "PDF URL: https://arxiv.org/pdf/2402.09413\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for block in paper_blocks:\n",
    "    # Find the <dd> tag that immediately follows each <dt> tag\n",
    "    metadata = block.find_next_sibling('dd')\n",
    "    title = metadata.find('div', class_='list-title').text.replace('Title:', '').strip()\n",
    "    authors = [a.text for a in metadata.find('div', class_='list-authors').find_all('a')]\n",
    "    abstract = metadata.find('p').text.strip()\n",
    "    # Extract the PDF link from the <dt> block\n",
    "    pdf_link_suffix = block.find('a', title='Download PDF')['href']\n",
    "    pdf_url = f'https://arxiv.org{pdf_link_suffix}'\n",
    "    print(f'Title: {title}\\nAuthors: {\", \".join(authors)}\\nAbstract: {abstract}\\nPDF URL: {pdf_url}\\n{\"-\"*40}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m abstract \u001b[38;5;241m=\u001b[39m paper\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Navigate to the previous sibling to find the PDF link\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m list_identifier \u001b[38;5;241m=\u001b[39m \u001b[43mpaper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_previous_sibling\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlist-identifier\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m pdf_link_suffix \u001b[38;5;241m=\u001b[39m list_identifier\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownload PDF\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m pdf_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://arxiv.org\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_link_suffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "for paper in papers:\n",
    "    title = paper.find('div', class_='list-title').text.replace('Title:', '').strip()\n",
    "    authors = [a.text for a in paper.find('div', class_='list-authors').find_all('a')]\n",
    "    abstract = paper.find('p').text.strip()\n",
    "    # Navigate to the previous sibling to find the PDF link\n",
    "    list_identifier = paper.find_previous_sibling('dt').find('span', class_='list-identifier')\n",
    "    pdf_link_suffix = list_identifier.find('a', title='Download PDF')['href']\n",
    "    pdf_url = f'https://arxiv.org{pdf_link_suffix}'\n",
    "    print(f'Title: {title}\\nAuthors: {\", \".join(authors)}\\nAbstract: {abstract}\\nURL: {url}\\n{\"-\"*40}')\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Binfeng Xu\n",
      "Affiliation: New York University\n",
      "Interests: ['Machine Learning']\n",
      "Cited by: 36\n",
      "h-index: 2\n",
      "i10-index: 2\n",
      "Number of publications: 3\n"
     ]
    }
   ],
   "source": [
    "from scholarly import scholarly\n",
    "\n",
    "# Replace 'Joseph Y. Halpern' with the name of the author you are interested in\n",
    "author_name = 'Binfeng Xu'\n",
    "\n",
    "try:\n",
    "    # Search for the author and take the first result\n",
    "    search_query = scholarly.search_author(author_name)\n",
    "    author = next(search_query)\n",
    "    \n",
    "    # Fill in more detailed information about the author\n",
    "    scholarly.fill(author, sections=['basics', 'indices', 'counts', 'publications'])\n",
    "    \n",
    "    print(f\"Name: {author['name']}\")\n",
    "    print(f\"Affiliation: {author.get('affiliation')}\")\n",
    "    print(f\"Interests: {author.get('interests', [])}\")\n",
    "    print(f\"Cited by: {author['citedby']}\")\n",
    "    print(f\"h-index: {author['hindex']}\")\n",
    "    print(f\"i10-index: {author['i10index']}\")\n",
    "    print(f\"Number of publications: {len(author['publications'])}\")\n",
    "except StopIteration:\n",
    "    print(\"Author not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author.get(\"citedby\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arxplore.parsers import parse_arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 156 papers in the cs.AI section of arXiv.\n",
      "\n",
      "Parsing information for Sujay Nagesh Koujalgi from Google Scholar...\n",
      "Parsing information for Jonathan Dodge from Google Scholar...\n",
      "Parsing information for Lance Ying from Google Scholar...\n",
      "Parsing information for Tan Zhi-Xuan from Google Scholar...\n",
      "Parsing information for Lionel Wong from Google Scholar...\n",
      "Parsing information for Vikash Mansinghka from Google Scholar...\n",
      "Parsing information for Joshua Tenenbaum from Google Scholar...\n",
      "Parsing information for Yiwen Sun from Google Scholar...\n",
      "Parsing information for Xianyin Zhang from Google Scholar...\n",
      "Parsing information for Shiyu Huang from Google Scholar...\n",
      "Parsing information for Shaowei Cai from Google Scholar...\n",
      "Parsing information for Bing-Zhen Zhang from Google Scholar...\n",
      "Parsing information for Ke Wei from Google Scholar...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Feed(section='cs.AI', pdf_url='https://arxiv.org/pdf/2402.10290', title='Experiments with Encoding Structured Data for Neural Networks', authors=[Author(name='Sujay Nagesh Koujalgi', affiliation='', interests=[], citation=0, h_index=0, n_publications=0), Author(name='Jonathan Dodge', affiliation='Assistant Professor, Penn State University', interests=['Explainable AI', 'Human-Computer Interaction', 'Graphics'], citation=1120, h_index=12, n_publications=33)], f_author=Author(name='Sujay Nagesh Koujalgi', affiliation='', interests=[], citation=0, h_index=0, n_publications=0), abstract=\"The project's aim is to create an AI agent capable of selecting good actions\\nin a game-playing domain called Battlespace. Sequential domains like\\nBattlespace are important testbeds for planning problems, as such, the\\nDepartment of Defense uses such domains for wargaming exercises. The agents we\\ndeveloped combine Monte Carlo Tree Search (MCTS) and Deep Q-Network (DQN)\\ntechniques in an effort to navigate the game environment, avoid obstacles,\\ninteract with adversaries, and capture the flag. This paper will focus on the\\nencoding techniques we explored to present complex structured data stored in a\\nPython class, a necessary precursor to an agent.\"),\n",
       " Feed(section='cs.AI', pdf_url='https://arxiv.org/pdf/2402.10416', title='Grounding Language about Belief in a Bayesian Theory-of-Mind', authors=[Author(name='Lance Ying', affiliation='Harvard University', interests=['Bayesian Modeling', 'Theory of Mind', 'Human-AI Interaction'], citation=8, h_index=2, n_publications=5), Author(name='Tan Zhi-Xuan', affiliation='MIT', interests=['Probabilistic Programming', 'Artificial intelligence', 'Computational Cognitive Science', 'Value Alignment'], citation=310, h_index=8, n_publications=19), Author(name='Lionel Catherine Wong', affiliation='Massachusetts Institute of Technology', interests=[], citation=799, h_index=12, n_publications=31), Author(name='Vikash K. Mansinghka', affiliation='MIT, Probabilistic Computing Project', interests=['artificial intelligence', 'statistics', 'probabilistic programming', 'machine learning'], citation=4551, h_index=28, n_publications=156), Author(name='Joshua B. Tenenbaum', affiliation='MIT', interests=['Cognitive science', 'artificial intelligence', 'machine learning', 'computational neuroscience', 'cognitive psychology'], citation=104081, h_index=138, n_publications=1016)], f_author=Author(name='Lance Ying', affiliation='Harvard University', interests=['Bayesian Modeling', 'Theory of Mind', 'Human-AI Interaction'], citation=8, h_index=2, n_publications=5), abstract=\"Despite the fact that beliefs are mental states that cannot be directly\\nobserved, humans talk about each others' beliefs on a regular basis, often\\nusing rich compositional language to describe what others think and know. What\\nexplains this capacity to interpret the hidden epistemic content of other\\nminds? In this paper, we take a step towards an answer by grounding the\\nsemantics of belief statements in a Bayesian theory-of-mind: By modeling how\\nhumans jointly infer coherent sets of goals, beliefs, and plans that explain an\\nagent's actions, then evaluating statements about the agent's beliefs against\\nthese inferences via epistemic logic, our framework provides a conceptual role\\nsemantics for belief, explaining the gradedness and compositionality of human\\nbelief attributions, as well as their intimate connection with goals and plans.\\nWe evaluate this framework by studying how humans attribute goals and beliefs\\nwhile watching an agent solve a doors-and-keys gridworld puzzle that requires\\ninstrumental reasoning about hidden objects. In contrast to pure logical\\ndeduction, non-mentalizing baselines, and mentalizing that ignores the role of\\ninstrumental plans, our model provides a much better fit to human goal and\\nbelief attributions, demonstrating the importance of theory-of-mind for a\\nsemantics of belief.\"),\n",
       " Feed(section='cs.AI', pdf_url='https://arxiv.org/pdf/2402.10705', title='AutoSAT: Automatically Optimize SAT Solvers via Large Language Models', authors=[Author(name='Yiwen Zhang', affiliation='Sun Yat-sen University', interests=['Immunology', 'Virology'], citation=1605, h_index=16, n_publications=32), Author(name='Xianyin Zhang', affiliation='', interests=[], citation=0, h_index=0, n_publications=0), Author(name='Shi-Yu Huang', affiliation='Professor of Eletrical Engineering, National Tsing Hua University', interests=['VLSI Testing', 'IC Design', 'Design Automation'], citation=3404, h_index=31, n_publications=358), Author(name='Shaowei Cai', affiliation='Institute of Software, Chinese Academy of Sciences', interests=['Satisfiability', 'Constraint Solving', 'Combinatorial Optimization', 'Heuristic Search'], citation=3040, h_index=33, n_publications=136), Author(name='Bing-Zhen Zhang', affiliation='', interests=[], citation=0, h_index=0, n_publications=0), Author(name='Kewei Chen', affiliation=\"Banner Alzheimer's Institute, Phoenix, AZ\", interests=[\"Alzheimer's\", 'neuroimage', 'MRI', 'PET', 'statistics'], citation=41357, h_index=106, n_publications=851)], f_author=Author(name='Yiwen Zhang', affiliation='Sun Yat-sen University', interests=['Immunology', 'Virology'], citation=1605, h_index=16, n_publications=32), abstract='Heuristics are crucial in SAT solvers, while no heuristic rules are suitable\\nfor all problem instances. Therefore, it typically requires to refine specific\\nsolvers for specific problem instances. In this context, we present AutoSAT, a\\nnovel framework for automatically optimizing heuristics in SAT solvers. AutoSAT\\nis based on Large Large Models (LLMs) which is able to autonomously generate\\ncode, conduct evaluation, then utilize the feedback to further optimize\\nheuristics, thereby reducing human intervention and enhancing solver\\ncapabilities. AutoSAT operates on a plug-and-play basis, eliminating the need\\nfor extensive preliminary setup and model training, and fosters a Chain of\\nThought collaborative process with fault-tolerance, ensuring robust heuristic\\noptimization. Extensive experiments on a Conflict-Driven Clause Learning (CDCL)\\nsolver demonstrates the overall superior performance of AutoSAT, especially in\\nsolving some specific SAT problem instances.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_arxiv(\"cs.AI\", tests = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database already exists!\n"
     ]
    }
   ],
   "source": [
    "from arxplorer.db import init_db\n",
    "from arxplorer.parsers import _parse_scholar\n",
    "init_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Author(name='Binfeng Xu', affiliation='New York University', interests='Machine Learning', citation=36, h_index=2, n_publications=3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_parse_scholar(\"Binfeng Xu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Author(name='Binfeng Xu', affiliation='New York University', interests='Machine Learning', citation=36, h_index=2, n_publications=3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_parse_scholar(\"Binfeng Xu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'authors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([(a\u001b[38;5;241m.\u001b[39mh_index \u001b[38;5;241m-\u001b[39m avg) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed\u001b[38;5;241m.\u001b[39mauthors]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed\u001b[38;5;241m.\u001b[39mauthors)\n\u001b[1;32m     37\u001b[0m fe \u001b[38;5;241m=\u001b[39m FeatureExtractor([])\n\u001b[0;32m---> 38\u001b[0m \u001b[43mfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg_authors_citation\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m, in \u001b[0;36mFeatureExtractor.avg_authors_citation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mavg_authors_citation\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([a\u001b[38;5;241m.\u001b[39mcitation \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthors\u001b[49m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed\u001b[38;5;241m.\u001b[39mauthors)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'authors'"
     ]
    }
   ],
   "source": [
    "# Defining useful signals for ranking\n",
    "\n",
    "from arxplorer.datamodel import Feed\n",
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, feeds: list[Feed]):\n",
    "        self.feed = feeds\n",
    "\n",
    "    @property\n",
    "    def first_author_citation(self) -> int:\n",
    "        return self.feed.authors[0].citation\n",
    "\n",
    "    @property\n",
    "    def avg_authors_citation(self) -> float:\n",
    "        return sum([a.citation for a in self.feed.authors]) / len(self.feed.authors)\n",
    "\n",
    "    @property\n",
    "    def variance_authors_citation(self) -> float:\n",
    "        avg = self.avg_authors_citation\n",
    "        return sum([(a.citation - avg) ** 2 for a in self.feed.authors]) / len(self.feed.authors)\n",
    "\n",
    "    @property\n",
    "    def first_author_h_index(self) -> int:\n",
    "        return self.feed.authors[0].h_index\n",
    "\n",
    "    @property\n",
    "    def avg_authors_h_index(self) -> float:\n",
    "        return sum([a.h_index for a in self.feed.authors]) / len(self.feed.authors)\n",
    "\n",
    "    @property\n",
    "    def variance_authors_h_index(self) -> float:\n",
    "        avg = self.avg_authors_h_index\n",
    "        return sum([(a.h_index - avg) ** 2 for a in self.feed.authors]) / len(self.feed.authors)\n",
    "    \n",
    "\n",
    "fe = FeatureExtractor([])\n",
    "fe.avg_authors_citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 497.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 323.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 608.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "ins = \"I like papers with innovative ideas instead of replication of existing methods on subfields. World modeling, planning and automation interest me most while others are also welcome.\"\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(ins)\n",
    "embedding2 = model.encode(\"Robotics\")\n",
    "embedding3 = model.encode(\"Machine Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1791422797013742"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from arxplorer.utils import *\n",
    "from arxplorer.utils import embedding_L2_similarity\n",
    "\n",
    "embedding_L2_similarity(embeddings, embedding2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'A', 'C']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rankings for each feature (the index represents the paper, values represent rankings)\n",
    "F1_rankings = ['B', 'A', 'C']  # F1 ranks A as the highest, followed by B, then C\n",
    "F2_rankings = ['B', 'C', 'A']  # F2 ranks B as the highest, followed by C, then A\n",
    "\n",
    "# Weights for each feature (reflecting their importance)\n",
    "weights = {'F1': 2, 'F2': 1}\n",
    "\n",
    "# List of papers\n",
    "papers = ['A', 'B', 'C']\n",
    "\n",
    "\n",
    "def calculate_weighted_copeland_scores(rankings, weights, papers):\n",
    "    scores = {paper: 0 for paper in papers}  # Initialize scores for each paper\n",
    "    \n",
    "    # Function to get rank of a paper in a feature\n",
    "    def get_rank(paper, feature_rankings):\n",
    "        return feature_rankings.index(paper)\n",
    "    \n",
    "    # Perform pairwise comparisons\n",
    "    for i in range(len(papers)):\n",
    "        for j in range(i + 1, len(papers)):\n",
    "            paper1, paper2 = papers[i], papers[j]\n",
    "            for feature, ranking in rankings.items():\n",
    "                rank1, rank2 = get_rank(paper1, ranking), get_rank(paper2, ranking)\n",
    "                weight = weights[feature]\n",
    "                if rank1 < rank2:  # paper1 is ranked higher than paper2\n",
    "                    scores[paper1] += weight\n",
    "                    scores[paper2] -= weight\n",
    "                elif rank1 > rank2:  # paper2 is ranked higher than paper1\n",
    "                    scores[paper1] -= weight\n",
    "                    scores[paper2] += weight\n",
    "                # Ties are ignored in this implementation\n",
    "\n",
    "    sorted_papers = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [paper for paper, _ in sorted_papers]\n",
    "\n",
    "# Combine rankings into a single dictionary for easier processing\n",
    "rankings = {'F1': F1_rankings, 'F2': F2_rankings}\n",
    "\n",
    "calculate_weighted_copeland_scores(rankings, weights, papers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
